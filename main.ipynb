{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using DBLP dataset from original TaxoGen paper\n",
    "\n",
    "terms = []\n",
    "documents = []\n",
    "embeddings = {}\n",
    "\n",
    "with open('input/keywords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        terms.append(line.strip())\n",
    "\n",
    "with open('input/documents.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        documents.append(line.strip().split())\n",
    "    \n",
    "with open('input/embeddings.txt', 'r') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        embeddings[parts[0]] = np.array(list(map(float, parts[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1000 random documents from the original documents list\n",
    "sample_documents = random.sample(documents, k=10000)\n",
    "\n",
    "# Get the terms that are present in the selected documents\n",
    "sample_terms = list(set([term for doc in sample_documents for term in doc if term in terms]))\n",
    "\n",
    "# Filter the embeddings dictionary to only include the terms in the selected documents\n",
    "sample_embeddings = {term: embedding for term, embedding in embeddings.items() if term in sample_terms}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building taxonomy at level 1, node \"root\"\n",
      "adaptive clustering iteration № 0\n",
      "performing clustering...\n",
      "assigning documents to clusters...\n",
      "Assigning document 0 out of 10000\n",
      "Assigning document 1000 out of 10000\n",
      "Assigning document 2000 out of 10000\n",
      "Assigning document 3000 out of 10000\n",
      "Assigning document 4000 out of 10000\n",
      "Assigning document 5000 out of 10000\n",
      "Assigning document 6000 out of 10000\n",
      "Assigning document 7000 out of 10000\n",
      "Assigning document 8000 out of 10000\n",
      "Assigning document 9000 out of 10000\n",
      "calculating representativeness scores...\n",
      "Building taxonomy at level 2, node \"aes\"\n",
      "adaptive clustering iteration № 0\n",
      "performing clustering...\n",
      "assigning documents to clusters...\n",
      "Assigning document 0 out of 10000\n",
      "Assigning document 1000 out of 10000\n",
      "Assigning document 2000 out of 10000\n",
      "Assigning document 3000 out of 10000\n",
      "Assigning document 4000 out of 10000\n",
      "Assigning document 5000 out of 10000\n",
      "Assigning document 6000 out of 10000\n",
      "Assigning document 7000 out of 10000\n",
      "Assigning document 8000 out of 10000\n",
      "Assigning document 9000 out of 10000\n",
      "calculating representativeness scores...\n",
      "Building taxonomy at level 2, node \"image_sequences\"\n",
      "adaptive clustering iteration № 0\n",
      "performing clustering...\n",
      "assigning documents to clusters...\n",
      "Assigning document 0 out of 10000\n",
      "Assigning document 1000 out of 10000\n",
      "Assigning document 2000 out of 10000\n",
      "Assigning document 3000 out of 10000\n",
      "Assigning document 4000 out of 10000\n",
      "Assigning document 5000 out of 10000\n",
      "Assigning document 6000 out of 10000\n",
      "Assigning document 7000 out of 10000\n",
      "Assigning document 8000 out of 10000\n",
      "Assigning document 9000 out of 10000\n",
      "calculating representativeness scores...\n",
      "Building taxonomy at level 2, node \"team\"\n",
      "adaptive clustering iteration № 0\n",
      "performing clustering...\n",
      "assigning documents to clusters...\n",
      "Assigning document 0 out of 10000\n",
      "Assigning document 1000 out of 10000\n",
      "Assigning document 2000 out of 10000\n",
      "Assigning document 3000 out of 10000\n",
      "Assigning document 4000 out of 10000\n",
      "Assigning document 5000 out of 10000\n",
      "Assigning document 6000 out of 10000\n",
      "Assigning document 7000 out of 10000\n",
      "Assigning document 8000 out of 10000\n",
      "Assigning document 9000 out of 10000\n",
      "calculating representativeness scores...\n",
      "Building taxonomy at level 2, node \"active_learning\"\n",
      "adaptive clustering iteration № 0\n",
      "performing clustering...\n",
      "assigning documents to clusters...\n",
      "Assigning document 0 out of 10000\n",
      "Assigning document 1000 out of 10000\n",
      "Assigning document 2000 out of 10000\n",
      "Assigning document 3000 out of 10000\n",
      "Assigning document 4000 out of 10000\n",
      "Assigning document 5000 out of 10000\n",
      "Assigning document 6000 out of 10000\n",
      "Assigning document 7000 out of 10000\n",
      "Assigning document 8000 out of 10000\n",
      "Assigning document 9000 out of 10000\n",
      "calculating representativeness scores...\n",
      "Building taxonomy at level 2, node \"rdf\"\n",
      "adaptive clustering iteration № 0\n",
      "performing clustering...\n",
      "assigning documents to clusters...\n",
      "Assigning document 0 out of 10000\n",
      "Assigning document 1000 out of 10000\n",
      "Assigning document 2000 out of 10000\n",
      "Assigning document 3000 out of 10000\n",
      "Assigning document 4000 out of 10000\n",
      "Assigning document 5000 out of 10000\n",
      "Assigning document 6000 out of 10000\n",
      "Assigning document 7000 out of 10000\n",
      "Assigning document 8000 out of 10000\n",
      "Assigning document 9000 out of 10000\n",
      "calculating representativeness scores...\n"
     ]
    }
   ],
   "source": [
    "from main import main\n",
    "root = main(sample_documents, sample_terms, sample_embeddings, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root    \n",
      "├── encryption_scheme ['encryption_scheme', 'adversary', 'cipher', 'authentication_protocol', 'key_management']\n",
      "│   ├── wiretap ['wiretap', 'smart_cards', 'arithmetic', 'invited_talk', 'backbone']\n",
      "│   ├── calculus ['calculus', 'overlay', 'overlay_network', 'weak', 'acquaintances']\n",
      "│   ├── cellular_automata ['cellular_automata', 'watermarking', 'schedule', 'cryptosystem', 'congestion']\n",
      "│   ├── identity_based ['identity_based', 'information_flow_control', 'two_party', 'noninterference', 'signature_schemes']\n",
      "│   └── nondeterministic ['nondeterministic', 'network_architecture', 'equivalence', 'router', 'routers']\n",
      "├── silhouettes ['silhouettes', 'input_image', 'cameras', 'based_on_wavelet', 'shape_and_texture']\n",
      "│   ├── image_registration ['image_registration', 'binarization', 'image_sequences', 'obstacle_avoidance', 'texture_classification']\n",
      "│   ├── dpca ['dpca', 'detection_and_classification', 'motion_tracking', 'aligning', 'biometrics']\n",
      "│   ├── image_databases ['image_databases', 'imaging', 'articulated', 'color', 'motion_capture']\n",
      "│   ├── super_resolution ['super_resolution', 'color_images', 'head_pose', 'gait_recognition', 'humanoid_robot']\n",
      "│   └── iris ['iris', 'event_detection', 'floorplanning', 'visual_attention', 'frontal']\n",
      "├── team ['team', 'e_business', 'problem_solving', 'service_oriented', 'resource_allocation']\n",
      "│   ├── service_composition ['service_composition', 'reflexes', 'trust_and_reputation', 'virtual_reality', 'continual']\n",
      "│   ├── user_modelling ['user_modelling', 'logistics', 'home_care', 'design', 'assistants']\n",
      "│   ├── model_driven_development ['model_driven_development', 'web_technologies', 'web_accessibility', 'e_government', 'pervasive']\n",
      "│   ├── human_computer_interaction ['human_computer_interaction', 'informatics', 'htn_planning', 'electronic_government', 'hci']\n",
      "│   └── bdi ['bdi', 'crew', 'human_like', 'ontology_development', 'multi_domain']\n",
      "├── regression_model ['regression_model', 'dimensionality_reduction', 'averaging', 'supervised_learning', 'anns']\n",
      "│   ├── xcs ['xcs', 'multiclass', 'kernel_pca', 'reweighting', 'ascsl']\n",
      "│   ├── probabilistic_inference ['probabilistic_inference', 'conditioning', 'lvq', 'logistic', 'ant']\n",
      "│   ├── fuzzy_controller ['fuzzy_controller', 'regret', 'multi_class', 'subspace', 'gp']\n",
      "│   ├── spiking_neurons ['spiking_neurons', 'onn', 'walksat', 'basis_functions', 'winnow']\n",
      "│   └── pursuit ['pursuit', 'dynamical', 'sparsity', 'forecast', 'nonlinear_systems']\n",
      "└── search_engine ['search_engine', 'lexical', 'search_results', 'semantically', 'semantic_analysis']\n",
      "    ├── structured_data ['structured_data', 'ads', 'subsumption', 'z_eves', 'protein']\n",
      "    ├── wsml ['wsml', 'essay', 'sequential_patterns', 'keyword_based', 'cbr']\n",
      "    ├── emotion ['emotion', 'epal', 'sequence_comparison', 'owl_s', 'tree_structured_data']\n",
      "    ├── co_clustering ['co_clustering', 'user_profiles', 'clues', 'tables', 'mystiq']\n",
      "    └── genome ['genome', 'pattern_mining', 'citations', 'data', 'first_order_logic']\n"
     ]
    }
   ],
   "source": [
    "root.print_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
